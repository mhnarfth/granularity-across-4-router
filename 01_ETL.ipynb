{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ac6c32-212e-4ca3-9c74-d2424de2d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Environment Setup ---\n",
      "Pandas version: 2.2.3\n",
      "Dask version: 2025.5.1\n",
      "\n",
      "--- Configuration Loaded ---\n",
      "Input Raw Data Directory: /mnt/nrdstor/ramamurthy/mhnarfth/internet2/parquet\n",
      "Output Processed File Path: /mnt/nrdstor/ramamurthy/mhnarfth/internet2/journal_extension_data/journal_study_4r_57d_unaggregated.parquet\n",
      "Routers to be Processed (4): ['dallas', 'atlanta', 'elpaso', 'boston']\n",
      "--------------------------------\n",
      "\n",
      "--- Starting ETL Process ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing router: 'dallas'...\n",
      "  Step 1/4: Loading raw data from dallas.parquet...\n",
      "  Step 2/4: Cleaning data and adding 'router' column...\n",
      "  Step 3/4: Computing Dask DataFrame to Pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  25%|██▌       | 1/4 [00:40<02:01, 40.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 4/4: Success! Processed 149,784,626 rows for 'dallas'.\n",
      "\n",
      "[INFO] Processing router: 'atlanta'...\n",
      "  Step 1/4: Loading raw data from atlanta.parquet...\n",
      "  Step 2/4: Cleaning data and adding 'router' column...\n",
      "  Step 3/4: Computing Dask DataFrame to Pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  50%|█████     | 2/4 [00:48<00:42, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 4/4: Success! Processed 28,032,240 rows for 'atlanta'.\n",
      "\n",
      "[INFO] Processing router: 'elpaso'...\n",
      "  Step 1/4: Loading raw data from elpaso.parquet...\n",
      "  Step 2/4: Cleaning data and adding 'router' column...\n",
      "  Step 3/4: Computing Dask DataFrame to Pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers:  75%|███████▌  | 3/4 [00:51<00:13, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 4/4: Success! Processed 9,357,137 rows for 'elpaso'.\n",
      "\n",
      "[INFO] Processing router: 'boston'...\n",
      "  Step 1/4: Loading raw data from boston.parquet...\n",
      "  Step 2/4: Cleaning data and adding 'router' column...\n",
      "  Step 3/4: Computing Dask DataFrame to Pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Routers: 100%|██████████| 4/4 [00:51<00:00, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 4/4: Success! Processed 392,891 rows for 'boston'.\n",
      "\n",
      "--- All routers processed. ---\n",
      "\n",
      "--- Consolidating data from all processed routers... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully concatenated data. Final DataFrame contains 187,566,894 total rows.\n",
      "  Final DataFrame memory usage: 14.85 GB\n",
      "\n",
      "--- Final Consolidated DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 187566894 entries, 0 to 187566893\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Dtype         \n",
      "---  ------      -----         \n",
      " 0   timestamp   datetime64[ns]\n",
      " 1   in_packets  int64         \n",
      " 2   router      object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(1)\n",
      "memory usage: 4.2+ GB\n",
      "\n",
      "--- Data Preview ---\n",
      "                timestamp  in_packets  router\n",
      "0 2021-10-08 00:03:46.560        5000  dallas\n",
      "1 2021-10-08 00:00:56.640      170000  dallas\n",
      "2 2021-10-08 00:02:55.616        5000  dallas\n",
      "3 2021-10-08 00:01:39.840        5000  dallas\n",
      "4 2021-10-08 00:03:04.576       10000  dallas\n",
      "\n",
      "--- Saving final 'golden source' file... ---\n",
      "\n",
      "✅ SUCCESS! ✅\n",
      "Golden source file was saved successfully to:\n",
      "/mnt/nrdstor/ramamurthy/mhnarfth/internet2/journal_extension_data/journal_study_4r_57d_unaggregated.parquet\n",
      "\n",
      "--- Notebook 01_ETL.ipynb Complete ---\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  notebooks/01_ETL.ipynb\n",
    "#\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# Notebook 1: Data ETL (Extraction, Transformation, Loading)\n",
    "#\n",
    "# --- PURPOSE ---\n",
    "# This notebook performs the foundational data preparation for the entire study.\n",
    "# Its sole purpose is to:\n",
    "#   1. Load the raw, un-aggregated NetFlow data for a specific set of routers.\n",
    "#   2. Perform essential cleaning (data types, handling missing values).\n",
    "#   3. Consolidate the data from all selected routers into a single DataFrame.\n",
    "#   4. Save this single, clean \"golden source\" file back to NRDstor.\n",
    "#\n",
    "# --- WORKFLOW ---\n",
    "# This notebook should be run ONLY ONCE for the entire project. Its output is the\n",
    "# primary input for all subsequent notebooks.\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# CELL 1: ENVIRONMENT SETUP & CONFIGURATION (KNOBS)\n",
    "#######################################################################\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import gc # For memory management\n",
    "\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "\n",
    "print(\"--- Environment Setup ---\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Dask version: {dask.__version__}\")\n",
    "\n",
    "# --- Suppress Warnings for Cleaner Output ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# >>> CONFIGURATION KNOBS <<<\n",
    "# Adjust these parameters to control the ETL process.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# --- Input Path ---\n",
    "# Directory on NRDstor containing the original raw Parquet files (one per router).\n",
    "INPUT_RAW_DATA_DIR = Path(\"/mnt/nrdstor/ramamurthy/mhnarfth/internet2/parquet\")\n",
    "\n",
    "# --- Output Paths ---\n",
    "# Directory on NRDstor where the final processed \"golden source\" file will be saved.\n",
    "OUTPUT_PROCESSED_DIR = Path(\"/mnt/nrdstor/ramamurthy/mhnarfth/internet2/journal_extension_data/\")\n",
    "OUTPUT_FILENAME = \"journal_study_4r_57d_unaggregated.parquet\"\n",
    "\n",
    "# --- Router Selection ---\n",
    "# The specific set of routers to process for this study.\n",
    "# Ensure names are lowercase to match the filenames (e.g., 'dallas.parquet').\n",
    "ROUTERS_TO_PROCESS = [\n",
    "    'dallas',\n",
    "    'atlanta',\n",
    "    'elpaso',\n",
    "    'boston'\n",
    "]\n",
    "\n",
    "# --- Column Selection ---\n",
    "# The essential columns to keep from the raw data.\n",
    "COLUMNS_TO_KEEP = ['t_first', 'in_packets']\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# --- Configuration Verification ---\n",
    "# ---------------------------------------------------------------------------------\n",
    "PROCESSED_DATA_PATH = OUTPUT_PROCESSED_DIR / OUTPUT_FILENAME\n",
    "\n",
    "print(\"\\n--- Configuration Loaded ---\")\n",
    "print(f\"Input Raw Data Directory: {INPUT_RAW_DATA_DIR}\")\n",
    "print(f\"Output Processed File Path: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"Routers to be Processed ({len(ROUTERS_TO_PROCESS)}): {ROUTERS_TO_PROCESS}\")\n",
    "print(\"--------------------------------\\n\")\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# CELL 2: ETL PROCESSING LOOP\n",
    "#######################################################################\n",
    "\n",
    "print(\"--- Starting ETL Process ---\")\n",
    "\n",
    "# Initialize an empty list to hold the processed Pandas DataFrames for each router\n",
    "list_of_processed_dfs = []\n",
    "\n",
    "# Use tqdm for a progress bar over the router list\n",
    "for router_name in tqdm(ROUTERS_TO_PROCESS, desc=\"Processing Routers\"):\n",
    "    print(f\"\\n[INFO] Processing router: '{router_name}'...\")\n",
    "    \n",
    "    # Construct the full path to the input file for the current router\n",
    "    input_file_path = INPUT_RAW_DATA_DIR / f\"{router_name}.parquet\"\n",
    "\n",
    "    if not os.path.exists(input_file_path):\n",
    "        print(f\"  [WARNING] File not found for router '{router_name}' at: {input_file_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load the raw data using Dask for memory efficiency\n",
    "        print(f\"  Step 1/4: Loading raw data from {input_file_path.name}...\")\n",
    "        ddf = dd.read_parquet(input_file_path, columns=COLUMNS_TO_KEEP)\n",
    "\n",
    "        # Step 2: Perform essential cleaning and transformation\n",
    "        print(\"  Step 2/4: Cleaning data and adding 'router' column...\")\n",
    "        # Convert timestamp column, coercing errors to NaT (Not a Time)\n",
    "        ddf['t_first'] = dd.to_datetime(ddf['t_first'], errors='coerce')\n",
    "        # Drop rows where the timestamp could not be parsed\n",
    "        ddf = ddf.dropna(subset=['t_first'])\n",
    "        # Add the router name as a new column for identification\n",
    "        ddf['router'] = router_name\n",
    "        \n",
    "        # Rename 't_first' to 'timestamp' for clarity in subsequent notebooks\n",
    "        ddf = ddf.rename(columns={'t_first': 'timestamp'})\n",
    "\n",
    "        # Step 3: Compute the Dask DataFrame to a Pandas DataFrame\n",
    "        # This is the point where the data is actually loaded into memory for this router.\n",
    "        print(\"  Step 3/4: Computing Dask DataFrame to Pandas...\")\n",
    "        pdf = ddf.compute()\n",
    "\n",
    "        # Step 4: Append the processed Pandas DataFrame to our list\n",
    "        if not pdf.empty:\n",
    "            list_of_processed_dfs.append(pdf)\n",
    "            print(f\"  Step 4/4: Success! Processed {len(pdf):,} rows for '{router_name}'.\")\n",
    "        else:\n",
    "            print(f\"  [WARNING] No valid data found for router '{router_name}' after processing. Skipping.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] An unexpected error occurred while processing router '{router_name}': {e}. Skipping.\")\n",
    "\n",
    "    # Explicitly clean up memory after each router is processed\n",
    "    del ddf\n",
    "    if 'pdf' in locals():\n",
    "        del pdf\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- All routers processed. ---\")\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# CELL 3: CONSOLIDATION AND SAVING\n",
    "#######################################################################\n",
    "\n",
    "print(\"\\n--- Consolidating data from all processed routers... ---\")\n",
    "\n",
    "if not list_of_processed_dfs:\n",
    "    print(\"[ERROR] No data was successfully processed. The 'list_of_processed_dfs' is empty. Exiting.\")\n",
    "else:\n",
    "    # Step 1: Concatenate all individual DataFrames into one master DataFrame\n",
    "    master_df = pd.concat(list_of_processed_dfs, ignore_index=True)\n",
    "    \n",
    "    # Free up memory by deleting the list of individual DataFrames\n",
    "    del list_of_processed_dfs\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"  Successfully concatenated data. Final DataFrame contains {len(master_df):,} total rows.\")\n",
    "    print(f\"  Final DataFrame memory usage: {master_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Display summary and a preview of the final dataset\n",
    "    print(\"\\n--- Final Consolidated DataFrame Info ---\")\n",
    "    master_df.info()\n",
    "    print(\"\\n--- Data Preview ---\")\n",
    "    print(master_df.head())\n",
    "    \n",
    "    # Step 2: Ensure the output directory exists\n",
    "    print(f\"\\n--- Saving final 'golden source' file... ---\")\n",
    "    try:\n",
    "        OUTPUT_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Step 3: Save the master DataFrame to a single Parquet file\n",
    "        # Parquet is efficient for storage and fast for loading later.\n",
    "        master_df.to_parquet(PROCESSED_DATA_PATH, index=False, engine='pyarrow', compression='snappy')\n",
    "        \n",
    "        print(f\"\\n✅ SUCCESS! ✅\")\n",
    "        print(f\"Golden source file was saved successfully to:\")\n",
    "        print(f\"{PROCESSED_DATA_PATH}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FATAL ERROR] Failed to save the final Parquet file: {e}\")\n",
    "        print(\"Please check your permissions and the path for the NRDstor directory.\")\n",
    "\n",
    "print(\"\\n--- Notebook 01_ETL.ipynb Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374a2f7d-9ab9-4635-8c4f-cf6ff2714815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying Data Distribution in Final DataFrame ---\n",
      "Row count per router in the final consolidated DataFrame:\n",
      "router\n",
      "dallas     149784626\n",
      "atlanta     28032240\n",
      "elpaso       9357137\n",
      "boston        392891\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[SUCCESS] Verification passed: The number of unique routers in the DataFrame matches the input list.\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# CELL 4: DATA VERIFICATION\n",
    "#######################################################################\n",
    "\n",
    "print(\"\\n--- Verifying Data Distribution in Final DataFrame ---\")\n",
    "\n",
    "if 'master_df' in locals() and not master_df.empty:\n",
    "    # Use value_counts() to get the number of rows for each unique router.\n",
    "    # This is the most direct way to confirm all routers are present.\n",
    "    router_counts = master_df['router'].value_counts()\n",
    "    \n",
    "    print(\"Row count per router in the final consolidated DataFrame:\")\n",
    "    print(router_counts)\n",
    "    \n",
    "    # Optional: A small check to confirm if the number of unique routers matches our input list\n",
    "    if len(router_counts) == len(ROUTERS_TO_PROCESS):\n",
    "        print(\"\\n[SUCCESS] Verification passed: The number of unique routers in the DataFrame matches the input list.\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] Verification mismatch: The number of unique routers does not match the input list.\")\n",
    "        print(f\"  Expected: {len(ROUTERS_TO_PROCESS)} routers\")\n",
    "        print(f\"  Found: {len(router_counts)} routers\")\n",
    "else:\n",
    "    print(\"[ERROR] 'master_df' not found or is empty. Cannot perform verification.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mahin's Thesis Venv",
   "language": "python",
   "name": "mahin_mthesis_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
